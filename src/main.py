import asyncio
import os
import sys
import datetime
from playwright.async_api import async_playwright
from urllib.parse import urlparse

from src.config import (
    INPUT_FILE, OUTPUT_DIR, HEADLESS_MODE, URL_TASK_TIMEOUT,
    MAXIMUM_CONCURRENCY_LIMIT, AUTO_SCALE, DOMAIN_CONCURRENCY_LIMIT
)
from src.scraper import Scraper
from src.queue_manager import QueueManager
from src.utils import load_start_urls, log
from src.load_scaler import LoadScaler


async def process_url(url, queue, scraper, browser):
    """Processes a URL and extracts new links."""
    log(f"ğŸ”„ Starting processing for: {url}")
    page = await browser.new_page()
    new_urls, success = [], False
    
    try:
        new_urls, success = await scraper.fetch_dynamic_content(url, page)
        log(f"âœ… Successfully fetched content for: {url}")
        await queue.add_urls(new_urls)
    except asyncio.TimeoutError:
        log(f"â³ Task timed out for URL: {url}", error=True)
    except Exception as e:
        log(f"âŒ Error processing {url}: {e}", error=True)
    finally:
        log(f"ğŸ”’ Closing page for: {url}")
        await page.close()
    
    return new_urls, success


async def process_tasks(queue, browser, scraper, load_scaler):
    """Processes tasks from the queue with retries within the same domain."""
    tasks = set()
    failed_urls = set()

    while not queue.is_empty() or tasks:
        while len(tasks) < load_scaler.get_concurrency() and not queue.is_empty():
            url = await queue.get()
            log(f"â³ Processing URL: {url}")
            task = asyncio.create_task(
                asyncio.wait_for(process_url(url, queue, scraper, browser), timeout=URL_TASK_TIMEOUT)
            )
            tasks.add(task)

        done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
        
        for task in done:
            try:
                new_urls, success = task.result()
                load_scaler.adjust_concurrency(success)
                if new_urls:
                    await queue.add_urls(new_urls)
                if not success:
                    failed_urls.add(url)
            except Exception as e:
                log(f"âš ï¸ Error processing URL: {e}", error=True)
                load_scaler.adjust_concurrency(False)
                failed_urls.add(url)

    return failed_urls


async def process_domain(url, browser, domain_semaphore):
    """Handles crawling for a single domain with concurrency control."""
    domain = urlparse(url).netloc
    domain_filename = domain.replace(".", "_")
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(OUTPUT_DIR, f"{domain_filename}_{timestamp}.txt")

    log(f"ğŸ” Processing domain: {domain}")
    log(f"ğŸ“‚ Output file for {url} -> {output_file}")

    queue = QueueManager()
    await queue.add_urls([url])
    log(f"ğŸ“ Added {url} to queue.")

    scraper = Scraper(output_file)
    load_scaler = LoadScaler(auto_scale=AUTO_SCALE, max_limit=MAXIMUM_CONCURRENCY_LIMIT)
    log(f"ğŸ”§ Initial concurrency set to: {load_scaler.get_concurrency()}")

    async with domain_semaphore:
        failed_links = await process_tasks(queue, browser, scraper, load_scaler)
        
        if failed_links:
            log(f"ğŸ”„ Retrying {len(failed_links)} failed URLs for domain - {domain}.")
            await queue.add_urls(failed_links)
            await process_tasks(queue, browser, scraper, load_scaler)  # Best effort retry

    log(f"âœ… Completed crawling for domain: {domain}")


async def main():
    """Main function to initialize and start the crawler."""
    log("ğŸš€ Starting crawler...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    start_urls = load_start_urls(INPUT_FILE)
    if not start_urls:
        log("âŒ No start URLs found. Exiting...", error=True)
        sys.exit(1)

    log(f"ğŸ“ Found {len(start_urls)} start URLs. Initializing scrapers per domain...")

    async with async_playwright() as p:
        log("ğŸŒ Launching browser...")
        browser = await p.chromium.launch(headless=HEADLESS_MODE)
        domain_concurrency_semaphore = asyncio.Semaphore(DOMAIN_CONCURRENCY_LIMIT)
        
        domain_tasks = [
            asyncio.create_task(process_domain(url, browser, domain_concurrency_semaphore))
            for url in start_urls
        ]

        log("ğŸš€ All domain scraping tasks started. Waiting for completion...")
        await asyncio.gather(*domain_tasks)

        log("ğŸ‰ Crawling completed. Closing browser.")
        await browser.close()

    log("âœ… All tasks finished. Exiting...")


if __name__ == "__main__":
    log("ğŸŸ¢ Starting main execution.")
    asyncio.run(main())
